{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting Up\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install necessary packages\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We begin by installing packages required for analysis and visualization.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "install.packages(\"tidyverse\")\ninstall.packages(\"tidytext\")\ninstall.packages(\"topicmodels\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"RColorBrewer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load libraries\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we can use the libraries, we have to load them into our\nenvironment.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "library(tidyverse)\nlibrary(tidytext)\nlibrary(ggplot2)\nlibrary(topicmodels)\nlibrary(RColorBrewer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load dataset\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data we will use is available for download as a CSV file from a\nsimple API created using Datasette. I have crafted a SQL query to get\njust the data that we need. This query will return a table containing\neach bill summary's unique ID in the database and its text.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "data <- read.csv(\"https://llc.herokuapp.com/summaries.csv?sql=select%0D%0A++rowid%2C%0D%0A++content_text%0D%0Afrom%0D%0A++files%0D%0Awhere%0D%0A++%22path%22+like+%2274_2_s%25%22%0D%0Aorder+by%0D%0A++path&_size=max\")\nhead(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A Little Exploration\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start by getting just the text of the bills.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "text <- data %>% select(content_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenizing\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The goal is to find a reasonable set of words to feed into the LDA\nalgorithm for topic modeling. We start by tokenizing the text of the\nbill summaries.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens <- text %>% unnest_tokens(word, content_text)\ndata(\"stop_words\")\ntokens <- tokens %>% anti_join(stop_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now we can take a quick look at the tokens that appear more than 100\ntimes in the bill summaries.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens %>%\n  count(word, sort = TRUE) %>% \n  filter(n > 100) %>% \n  mutate(word = reorder(word, n)) %>% \n  ggplot(aes(n, word)) +\n  geom_col() +\n  labs(y = NULL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There's a lot here that won't contribute to meaningful topic modeling,\nspecifically numbers and names of months. Let's get rid of those.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Removing numbers and months\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens <- tokens %>%\n  filter(!grepl('[0-9]', word))\n\nmonth_tokens <- tibble(month.name) %>% \n  unnest_tokens(word, month.name)\n\ntokens <- tokens %>% anti_join(month_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Token frequency without numbers and months\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens %>%\n  count(word, sort = TRUE) %>% \n  filter(n > 100) %>% \n  mutate(word = reorder(word, n)) %>% \n  ggplot(aes(n, word)) +\n  geom_col() +\n  labs(y = NULL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is better, but there's more we can do. First, let's remove so more\nwords that might clutter our model.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Additional filters\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "bill_types <- read.csv(\"https://llc.herokuapp.com/summaries.csv?sql=select+distinct+bill_type+from+actions&_size=max\")\nsponsors <- read.csv(\"https://llc.herokuapp.com/summaries.csv?sql=select+distinct+sponsor+from+actions+where+sponsor+is+not+null+and+sponsor+%21%3D+%22%22&_size=max\")\nactions <- read.csv(\"https://llc.herokuapp.com/summaries.csv?sql=select+distinct+action+from+actions+where+action+is+not+null+and+action+%21%3D+%22%22+and+action+%21%3D+%22N%2FA%22&_size=max\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### More tokens\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "bill_type_tokens <- bill_types %>% \n  unnest_tokens(word, bill_type)\nsponsor_tokens <- sponsors %>% \n  unnest_tokens(word, sponsor)\naction_tokens <- actions %>% \n  unnest_tokens(word, action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Removing bill type and sponsor tokens\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens <- tokens %>% \n  anti_join(bill_type_tokens) %>%\n  anti_join(sponsor_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another simple visualization\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Token frequency without bill types and sponsors\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens %>%\n  count(word, sort = TRUE) %>% \n  filter(n > 100) %>% \n  mutate(word = reorder(word, n)) %>% \n  ggplot(aes(n, word)) +\n  geom_col() +\n  labs(y = NULL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Not much has changed here, but words like \"approved\", \"reported\", and\n\"referred\" are terms for bill actions that won't contribute meaningfully\nto the topic modeling algorithm. We'll remove them next.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Removal:PROPERTIES:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "distinct_action_tokens <- action_tokens %>% \n  distinct(word) %>% \n  filter(!grepl('[0-9]', word)) %>% \n  filter(!grepl('ed$|ly$', word)) %>% \n  anti_join(stop_words)\n\naction_tokens_to_exclude <- action_tokens %>% \n  anti_join(distinct_action_tokens)\n\ntokens <- tokens %>% \n anti_join(action_tokens_to_exclude)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Token frequency without bill types, sponsors, months, and action\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "tokens to exclude\nAnother quick bar chart\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens %>%\n  count(word, sort = TRUE) %>% \n  filter(n > 100) %>% \n  mutate(word = reorder(word, n)) %>% \n  ggplot(aes(n, word)) +\n  geom_col() +\n  labs(y = NULL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is enough to get going on topic modeling.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Topic Modeling\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have to create a document term matrix for topic modeling. Luckily,\n`tidytext` makes that very easy to do.\n\nFirst, we get our tokens into the right shape for consumption by the\n`cast_dtm` function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens_for_dtm <- data %>% \n  unnest_tokens(word, content_text) %>% \n  filter(!grepl('[0-9]', word)) %>% \n  anti_join(stop_words) %>% \n  anti_join(month_tokens) %>% \n  anti_join(bill_type_tokens) %>%\n  anti_join(sponsor_tokens) %>% \n  anti_join(action_tokens_to_exclude) %>% \n  count(rowid, word) %>% \n  mutate(document = rowid, term = word, count = n) %>% \n  select(document, term, count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we create our document term matrix.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "senate_dtm <- tokens_for_dtm %>% \n  cast_dtm(document, term, count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now we can create our Latent Dirichlet Allocation object. k = 12 as\nan arbitrary choice\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "senate_lda <- LDA(senate_dtm, k = 12, control = list(seed= 1234))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can turn the topics generated by the LDA algorithm into tidy data for\nexploration.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "senate_topics <- tidy(senate_lda, matrix = \"beta\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we can pull out the top terms, based on the beta, or\n[per-topic-per-word\nprobabilities](https://www.tidytextmining.com/topicmodeling.html#word-topic-probabilities), for each term.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "senate_top_terms <- senate_topics %>% \n  group_by(topic) %>% \n  slice_max(beta, n = 10) %>% \n  ungroup() %>% \n  arrange(topic, -beta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see what we've got\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "senate_top_terms %>% \n  mutate(term = reorder_within(term, beta, topic)) %>% \n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\", ncol = 3) +\n  scale_y_reordered()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some high frequency terms, like \"act\", \"authorizes\", and \"public\" are\ncluttering our analysis. Let's sort the top terms by their beta and see\nwhat we can get rid of.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "top_terms_ordered <- senate_top_terms %>% arrange(desc(beta))\ntop_term_count <- top_terms_ordered %>% count(term) %>% arrange(desc(n))\ntop_term_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's remove the ones that show up more than 5 times.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "top_terms_to_remove <- top_term_count %>% filter(n > 5)\nsenate_top_terms <- senate_top_terms %>% anti_join(top_terms_to_remove)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now let's take another look.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "senate_top_terms %>% \n  mutate(term = reorder_within(term, beta, topic)) %>% \n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\", ncol = 3) +\n  scale_y_reordered()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Further analysis\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There were [48 Senate committees](https://en.wikipedia.org/wiki/74th_United_States_Congress#Committees) during the 74th Congress, so it could be interesting to change k to 48 in our call to `LDA()`.  However, that's pretty computationally intensive and would be hard to run in a free cloud computing environment like Google Colab.  It could also be interesting to visualize `LDA()` with k = 48 in [LDAvis](https://github.com/cpsievert/LDAvis).\n\n"
      ]
    }
  ],
  "metadata": {
    "org": null,
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "codemirror_mode": "r",
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.3.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}